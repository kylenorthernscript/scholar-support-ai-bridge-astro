#!/usr/bin/env python3
"""Verify current international opportunities using AMED search."""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urlencode
from datetime import datetime
import re

def search_amed_current():
    """Search AMED for current opportunities."""
    
    # Search for all current opportunities
    base_url = "https://www.amed.go.jp/search.php"
    
    # Parameters for current opportunities only
    params = {
        'keyword': '',
        'search': 'search',
        'order_by': '',
        'stage[]': 'ç¾åœ¨å…¬å‹Ÿä¸­'
    }
    
    headers = {
        "User-Agent": "Mozilla/5.0 (compatible; ResearchBot/1.0; +https://theta-tech.co.jp/bot)",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
    }
    
    # International keywords
    intl_keywords = [
        "å›½éš›", "æµ·å¤–", "ã‚°ãƒ­ãƒ¼ãƒãƒ«", "å¤–å›½", "å¤šå›½ç±",
        "ASPIRE", "e-ASIA", "SICORP", "Interstellar", "SATREPS",
        "æ—¥ç±³", "æ—¥æ¬§", "æ—¥ä¸­", "æ—¥ãƒ»", "æ—¥æœ¬ãƒ»",
        "å›½éš›å…±åŒ", "å›½éš›ç§‘å­¦", "å›½éš›å”åŠ›",
        "international", "global", "overseas"
    ]
    
    try:
        # Get all current opportunities
        print("ğŸ” ç¾åœ¨å…¬å‹Ÿä¸­ã®æ¡ˆä»¶ã‚’æ¤œç´¢ä¸­...")
        url = f"{base_url}?{urlencode(params, doseq=True)}"
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "lxml")
        
        # Find total count
        count_text = soup.find(text=re.compile(r'æ¤œç´¢çµæœ.*ä»¶ä¸­'))
        total_count = 0
        if count_text:
            match = re.search(r'(\d+)ä»¶ä¸­', str(count_text))
            if match:
                total_count = int(match.group(1))
        
        print(f"ğŸ“Š ç¾åœ¨å…¬å‹Ÿä¸­ã®ç·ä»¶æ•°: {total_count}ä»¶")
        
        # Now search for international opportunities
        print("\nğŸŒ å›½éš›é–¢é€£ã®å…¬å‹Ÿã‚’æ¤œç´¢ä¸­...")
        
        international_results = {}
        
        for keyword in ["å›½éš›", "æµ·å¤–", "ã‚°ãƒ­ãƒ¼ãƒãƒ«", "ASPIRE", "SICORP", "e-ASIA", "Interstellar"]:
            params['keyword'] = keyword
            url = f"{base_url}?{urlencode(params, doseq=True)}"
            
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, "lxml")
            
            # Find count
            count_text = soup.find(text=re.compile(r'æ¤œç´¢çµæœ.*ä»¶ä¸­'))
            if count_text:
                match = re.search(r'(\d+)ä»¶ä¸­', str(count_text))
                if match:
                    count = int(match.group(1))
                    if count > 0:
                        international_results[keyword] = count
                        print(f"  '{keyword}': {count}ä»¶")
        
        # Get unique international opportunities
        # (Note: This is an approximation as some may overlap)
        total_international = max(international_results.values()) if international_results else 0
        
        print(f"\nğŸ“ˆ åˆ†æçµæœ:")
        print(f"  ç¾åœ¨å…¬å‹Ÿä¸­: {total_count}ä»¶")
        print(f"  å›½éš›é–¢é€£ï¼ˆæ¨å®šï¼‰: {total_international}ä»¶")
        print(f"  å›½éš›ç ”ç©¶ç‡: {total_international/total_count*100:.1f}%" if total_count > 0 else "N/A")
        
        # Get details of international opportunities
        if total_international > 0:
            print(f"\nğŸ“‹ å›½éš›é–¢é€£å…¬å‹Ÿã®è©³ç´°:")
            
            # Search for "å›½éš›" to get the main international calls
            params['keyword'] = 'å›½éš›'
            url = f"{base_url}?{urlencode(params, doseq=True)}"
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, "lxml")
            
            # Find result items
            results = soup.find_all('div', class_='searchResultItem') or soup.find_all('li', class_='result')
            
            if not results:
                # Try alternative parsing
                links = soup.find_all('a', href=re.compile(r'/koubo/'))
                for i, link in enumerate(links[:10], 1):
                    title = link.get_text(strip=True)
                    if len(title) > 10:
                        print(f"\n{i}. {title[:100]}...")
                        
                        # Check which international keywords it contains
                        found_keywords = [kw for kw in intl_keywords if kw in title]
                        if found_keywords:
                            print(f"   ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(found_keywords[:5])}")
        
        return {
            'total_current': total_count,
            'international_estimated': total_international,
            'international_keywords': international_results
        }
        
    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()
        return None

def check_specific_international():
    """Check specific international programs."""
    programs = {
        'ASPIRE': 'å…ˆç«¯å›½éš›å…±åŒç ”ç©¶æ¨é€²ãƒ—ãƒ­ã‚°ãƒ©ãƒ ',
        'SICORP': 'æˆ¦ç•¥çš„å›½éš›å…±åŒç ”ç©¶ãƒ—ãƒ­ã‚°ãƒ©ãƒ ',
        'e-ASIA': 'e-ASIAå…±åŒç ”ç©¶ãƒ—ãƒ­ã‚°ãƒ©ãƒ ',
        'Interstellar': 'Interstellar Initiative',
        'SATREPS': 'åœ°çƒè¦æ¨¡èª²é¡Œå¯¾å¿œå›½éš›ç§‘å­¦æŠ€è¡“å”åŠ›ãƒ—ãƒ­ã‚°ãƒ©ãƒ '
    }
    
    print("\nğŸ” ä¸»è¦å›½éš›ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®ç¢ºèª:")
    
    for eng_name, jp_name in programs.items():
        print(f"\n{eng_name} ({jp_name}):")
        # This would require checking each program's status
        print(f"  â†’ æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã§ç¢ºèªãŒå¿…è¦")

if __name__ == "__main__":
    print("AMEDç¾åœ¨å…¬å‹Ÿä¸­ã®å›½éš›ç ”ç©¶æ©Ÿä¼š æ¤œè¨¼ãƒ„ãƒ¼ãƒ«")
    print("=" * 50)
    
    results = search_amed_current()
    
    if results:
        print("\n" + "=" * 50)
        print("âœ… æ¤œè¨¼å®Œäº†")
        print(f"\næœ€çµ‚çµæœ:")
        print(f"  ç¾åœ¨å…¬å‹Ÿä¸­ã®ç·æ•°: {results['total_current']}ä»¶")
        print(f"  ã†ã¡å›½éš›é–¢é€£: {results['international_estimated']}ä»¶ä»¥ä¸Š")
    
    check_specific_international()