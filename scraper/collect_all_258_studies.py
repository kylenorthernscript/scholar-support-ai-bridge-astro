#!/usr/bin/env python3
"""Collect all 258 international studies with contact information."""

import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import re
import time
import json
from urllib.parse import urlencode, urljoin
from openpyxl import Workbook
from openpyxl.styles import Font, PatternFill, Alignment
from openpyxl.utils.dataframe import dataframe_to_rows

class AMEDDataCollector:
    """Complete AMED international research data collector."""
    
    def __init__(self):
        self.base_url = "https://www.amed.go.jp/search.php"
        self.detail_cache = {}
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (compatible; ResearchBot/1.0)",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
        })
        
    def search_all_international(self):
        """Search for all international studies using multiple keywords."""
        all_studies = []
        
        # Multiple search strategies
        search_strategies = [
            {"keyword": "å›½éš›", "description": "General international"},
            {"keyword": "æµ·å¤–", "description": "Overseas"},
            {"keyword": "ã‚°ãƒ­ãƒ¼ãƒãƒ«", "description": "Global"},
            {"keyword": "ASPIRE", "description": "ASPIRE program"},
            {"keyword": "SICORP", "description": "SICORP program"},
            {"keyword": "e-ASIA", "description": "e-ASIA program"},
            {"keyword": "Interstellar", "description": "Interstellar Initiative"},
            {"keyword": "SATREPS", "description": "SATREPS program"},
            {"keyword": "å¤–å›½", "description": "Foreign"},
            {"keyword": "å¤šå›½ç±", "description": "Multinational"},
            {"keyword": "å…±åŒç ”ç©¶", "description": "Joint research"},
            {"keyword": "æ—¥ç±³", "description": "Japan-US"},
            {"keyword": "æ—¥æ¬§", "description": "Japan-Europe"},
            {"keyword": "æ—¥ä¸­", "description": "Japan-China"}
        ]
        
        unique_urls = set()
        
        for strategy in search_strategies:
            print(f"\nğŸ” æ¤œç´¢ä¸­: {strategy['description']} ('{strategy['keyword']}')")
            
            studies = self._search_by_keyword(strategy['keyword'])
            new_studies = 0
            
            for study in studies:
                if study['URL'] not in unique_urls:
                    unique_urls.add(study['URL'])
                    all_studies.append(study)
                    new_studies += 1
            
            print(f"  æ–°è¦ç™ºè¦‹: {new_studies}ä»¶")
            time.sleep(1)  # Rate limiting
        
        print(f"\nâœ… åé›†å®Œäº†: {len(all_studies)}ä»¶ã®å›½éš›ç ”ç©¶ã‚’ç™ºè¦‹")
        return all_studies
    
    def _search_by_keyword(self, keyword):
        """Search by specific keyword."""
        studies = []
        
        # Search both current and past
        params = {
            'keyword': keyword,
            'search': 'search',
            'order_by': '',
            'stage[]': ['ç¾åœ¨å…¬å‹Ÿä¸­', 'å…¬å‹Ÿæƒ…å ±ï¼ˆéå»ã®å…¬å‹Ÿæƒ…å ±ã‚‚å«ã‚€ï¼‰']
        }
        
        page = 1
        max_pages = 20  # Reasonable limit
        
        while page <= max_pages:
            try:
                current_params = params.copy()
                if page > 1:
                    current_params['page'] = page
                
                url = f"{self.base_url}?{urlencode(current_params, doseq=True)}"
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'lxml')
                
                # Look for results in page
                page_studies = self._extract_studies_from_page(soup, keyword)
                
                if not page_studies:
                    break
                
                studies.extend(page_studies)
                page += 1
                time.sleep(0.5)  # Rate limiting
                
            except Exception as e:
                print(f"    Error on page {page}: {e}")
                break
        
        return studies
    
    def _extract_studies_from_page(self, soup, keyword):
        """Extract studies from a search results page."""
        studies = []
        
        # Multiple patterns to find links
        link_patterns = [
            soup.find_all('a', href=re.compile(r'/koubo/')),
            soup.select('a[href*="koubo"]'),
            soup.find_all('a', href=re.compile(r'\.html'))
        ]
        
        found_links = set()
        for pattern in link_patterns:
            for link in pattern:
                href = link.get('href', '')
                if href:
                    found_links.add((link, href))
        
        for link, href in found_links:
            try:
                title = link.get_text(strip=True)
                
                # Filter criteria
                if (len(title) > 15 and 
                    'ä»¤å’Œ' in title and
                    any(intl_word in title.lower() for intl_word in 
                        ['å›½éš›', 'æµ·å¤–', 'ã‚°ãƒ­ãƒ¼ãƒãƒ«', 'aspire', 'sicorp', 'e-asia', 
                         'interstellar', 'satreps', 'å¤–å›½', 'å¤šå›½ç±', 'å…±åŒç ”ç©¶', 
                         'æ—¥ç±³', 'æ—¥æ¬§', 'æ—¥ä¸­']) and
                    not any(skip in href for skip in ['index', 'search', 'help', 'sitemap'])):
                    
                    full_url = urljoin("https://www.amed.go.jp", href)
                    
                    study = {
                        'ã‚¿ã‚¤ãƒˆãƒ«': title,
                        'URL': full_url,
                        'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰': keyword,
                        'ç™ºè¦‹ãƒšãƒ¼ã‚¸': soup.title.string if soup.title else ""
                    }
                    studies.append(study)
                    
            except Exception as e:
                continue
        
        return studies
    
    def enrich_with_details(self, studies):
        """Enrich studies with detailed information including contact details."""
        enriched_studies = []
        
        print(f"\nğŸ“‹ è©³ç´°æƒ…å ±ã‚’å–å¾—ä¸­... ({len(studies)}ä»¶)")
        
        for i, study in enumerate(studies, 1):
            print(f"  {i}/{len(studies)}: {study['ã‚¿ã‚¤ãƒˆãƒ«'][:50]}...")
            
            try:
                detailed_info = self._get_study_details(study['URL'])
                
                # Merge basic and detailed info
                enriched_study = {**study, **detailed_info}
                enriched_studies.append(enriched_study)
                
                # Rate limiting
                time.sleep(1)
                
            except Exception as e:
                print(f"    Error getting details: {e}")
                # Keep basic info even if details fail
                enriched_studies.append(study)
                continue
        
        return enriched_studies
    
    def _get_study_details(self, url):
        """Get detailed information from study page."""
        if url in self.detail_cache:
            return self.detail_cache[url]
        
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'lxml')
            text = soup.get_text()
            
            details = {
                'å¹´åº¦': self._extract_year(text),
                'ãƒ—ãƒ­ã‚°ãƒ©ãƒ ç¨®åˆ¥': self._extract_program_type(text),
                'å¯¾è±¡å›½': self._extract_countries(text),
                'ç ”ç©¶åˆ†é‡': self._extract_research_field(text),
                'ç· åˆ‡æ—¥': self._extract_deadline(text),
                'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹': self._extract_status(text),
                'äºˆç®—': self._extract_budget(text),
                'ç ”ç©¶æœŸé–“': self._extract_duration(text),
                'å®Ÿæ–½æ©Ÿé–¢': self._extract_implementing_agency(text),
                
                # Contact information
                'æ‹…å½“éƒ¨ç½²': self._extract_contact_department(text),
                'æ‹…å½“è€…': self._extract_contact_person(text),
                'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹': self._extract_email(text),
                'é›»è©±ç•ªå·': self._extract_phone(text),
                'å•ã„åˆã‚ã›å…ˆ': self._extract_inquiry_contact(text),
                
                # Additional details
                'å¿œå‹Ÿè³‡æ ¼': self._extract_eligibility(text),
                'é¸è€ƒæ–¹æ³•': self._extract_selection_method(text),
                'æå‡ºæ›¸é¡': self._extract_required_documents(text)
            }
            
            self.detail_cache[url] = details
            return details
            
        except Exception as e:
            print(f"    Failed to get details from {url}: {e}")
            return {}
    
    def _extract_year(self, text):
        """Extract year from text."""
        match = re.search(r'ä»¤å’Œ(\d+)å¹´', text)
        if match:
            return f"ä»¤å’Œ{match.group(1)}å¹´"
        return "ä¸æ˜"
    
    def _extract_program_type(self, text):
        """Extract program type."""
        program_patterns = {
            'ASPIRE': 'ASPIRE',
            'SICORP': 'SICORP',
            'e-ASIA': 'e-ASIA',
            'Interstellar': 'Interstellar Initiative',
            'SATREPS': 'SATREPS',
            'æ—¥ç±³åŒ»å­¦': 'æ—¥ç±³åŒ»å­¦å”åŠ›',
            'ã‚°ãƒ­ãƒ¼ãƒãƒ«å±•é–‹': 'ã‚°ãƒ­ãƒ¼ãƒãƒ«å±•é–‹',
            'æµ·å¤–æ‹ ç‚¹': 'æµ·å¤–æ‹ ç‚¹æ´»ç”¨',
            'ä»‹è­·.*æµ·å¤–': 'æµ·å¤–å±•é–‹'
        }
        
        for pattern, program in program_patterns.items():
            if re.search(pattern, text, re.IGNORECASE):
                return program
        
        return "ãã®ä»–å›½éš›ãƒ—ãƒ­ã‚°ãƒ©ãƒ "
    
    def _extract_countries(self, text):
        """Extract target countries."""
        country_patterns = {
            'æ—¥ãƒ»ã‚«ãƒŠãƒ€|æ—¥æœ¬.*ã‚«ãƒŠãƒ€': 'æ—¥æœ¬, ã‚«ãƒŠãƒ€',
            'æ—¥ãƒ»ã‚¹ã‚¤ã‚¹|æ—¥æœ¬.*ã‚¹ã‚¤ã‚¹': 'æ—¥æœ¬, ã‚¹ã‚¤ã‚¹',
            'æ—¥ãƒ»è‹±å›½|æ—¥æœ¬.*è‹±å›½|æ—¥ãƒ»ã‚¤ã‚®ãƒªã‚¹': 'æ—¥æœ¬, è‹±å›½',
            'æ—¥ãƒ»ãƒ•ãƒ©ãƒ³ã‚¹|æ—¥æœ¬.*ãƒ•ãƒ©ãƒ³ã‚¹': 'æ—¥æœ¬, ãƒ•ãƒ©ãƒ³ã‚¹',
            'æ—¥ãƒ»ãƒ‰ã‚¤ãƒ„|æ—¥æœ¬.*ãƒ‰ã‚¤ãƒ„': 'æ—¥æœ¬, ãƒ‰ã‚¤ãƒ„',
            'æ—¥ãƒ»ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢|æ—¥æœ¬.*ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢': 'æ—¥æœ¬, ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢',
            'æ—¥ç±³|æ—¥æœ¬.*ã‚¢ãƒ¡ãƒªã‚«|æ—¥æœ¬.*ç±³å›½': 'æ—¥æœ¬, ã‚¢ãƒ¡ãƒªã‚«',
            'e-ASIA|ã‚¢ã‚¸ã‚¢': 'æ—¥æœ¬, ã‚¢ã‚¸ã‚¢è«¸å›½',
            'ã‚¢ãƒ•ãƒªã‚«': 'ã‚¢ãƒ•ãƒªã‚«',
            'é–‹ç™ºé€”ä¸Šå›½': 'é–‹ç™ºé€”ä¸Šå›½',
            'æµ·å¤–': 'æµ·å¤–',
            'ã‚°ãƒ­ãƒ¼ãƒãƒ«|å›½éš›': 'å›½éš›'
        }
        
        for pattern, countries in country_patterns.items():
            if re.search(pattern, text, re.IGNORECASE):
                return countries
        
        return "è¤‡æ•°å›½/ä¸æ˜"
    
    def _extract_research_field(self, text):
        """Extract research field."""
        field_patterns = {
            'ãŒã‚“': 'ãŒã‚“ç ”ç©¶',
            'æ„ŸæŸ“ç—‡': 'æ„ŸæŸ“ç—‡ç ”ç©¶',
            'å†ç”ŸåŒ»ç™‚': 'å†ç”ŸåŒ»ç™‚',
            'åŒ»ç™‚æ©Ÿå™¨': 'åŒ»ç™‚æ©Ÿå™¨',
            'å‰µè–¬': 'å‰µè–¬',
            'ä»‹è­·': 'ä»‹è­·ãƒ»ç¦ç¥‰',
            'AI|äººå·¥çŸ¥èƒ½': 'AIãƒ»æƒ…å ±æŠ€è¡“',
            'è€åŒ–': 'è€åŒ–ãƒ»åŠ é½¢ç ”ç©¶',
            'å…ç–«': 'å…ç–«å­¦',
            'ç†±å¸¯ç—…': 'ç†±å¸¯ç—…',
            'åœ°çƒè¦æ¨¡': 'åœ°çƒè¦æ¨¡ä¿å¥'
        }
        
        for pattern, field in field_patterns.items():
            if re.search(pattern, text, re.IGNORECASE):
                return field
        
        return "åŒ»ç™‚æŠ€è¡“å…¨èˆ¬"
    
    def _extract_deadline(self, text):
        """Extract application deadline."""
        patterns = [
            r'ç· åˆ‡[æ—¥æ™‚]*[:ï¼š]\s*ä»¤å’Œ(\d+)å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
            r'å¿œå‹Ÿç· åˆ‡[:ï¼š]\s*ä»¤å’Œ(\d+)å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
            r'å…¬å‹Ÿç· åˆ‡[:ï¼š]\s*ä»¤å’Œ(\d+)å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
            r'ä»¤å’Œ(\d+)å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥.*ç· åˆ‡'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                year = 2018 + int(match.group(1))
                month = int(match.group(2))
                day = int(match.group(3))
                return f"{year}/{month:02d}/{day:02d}"
        
        return "ä¸æ˜"
    
    def _extract_status(self, text):
        """Extract current status."""
        if re.search(r'ç¾åœ¨.*å…¬å‹Ÿ|å‹Ÿé›†.*ä¸­', text):
            return "ç¾åœ¨å…¬å‹Ÿä¸­"
        elif re.search(r'ç· åˆ‡.*çµ‚äº†|å‹Ÿé›†.*çµ‚äº†', text):
            return "å‹Ÿé›†çµ‚äº†"
        else:
            # Determine by deadline if available
            deadline = self._extract_deadline(text)
            if deadline != "ä¸æ˜":
                try:
                    deadline_date = datetime.strptime(deadline, "%Y/%m/%d")
                    if deadline_date > datetime.now():
                        return "ç¾åœ¨å…¬å‹Ÿä¸­"
                    else:
                        return "å‹Ÿé›†çµ‚äº†"
                except:
                    pass
            return "è¦ç¢ºèª"
    
    def _extract_budget(self, text):
        """Extract budget information."""
        patterns = [
            r'äºˆç®—[:ï¼š]\s*([\d,]+)\s*ä¸‡å††',
            r'ç·é¡[:ï¼š]\s*([\d,]+)\s*ä¸‡å††',
            r'([\d,]+)\s*ä¸‡å††.*äºˆç®—',
            r'äºˆç®—.*?([\d,]+)\s*åƒå††'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return f"{match.group(1)}ä¸‡å††"
        
        return "ä¸æ˜"
    
    def _extract_duration(self, text):
        """Extract research duration."""
        patterns = [
            r'ç ”ç©¶æœŸé–“[:ï¼š]\s*(\d+)å¹´',
            r'æœŸé–“[:ï¼š]\s*(\d+)å¹´',
            r'(\d+)å¹´é–“'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return f"{match.group(1)}å¹´"
        
        return "ä¸æ˜"
    
    def _extract_implementing_agency(self, text):
        """Extract implementing agency."""
        agencies = {
            'AMED': 'AMED',
            'JST': 'JST',
            'JSPS': 'JSPS',
            'åšç”ŸåŠ´åƒçœ': 'åšç”ŸåŠ´åƒçœ',
            'æ–‡éƒ¨ç§‘å­¦çœ': 'æ–‡éƒ¨ç§‘å­¦çœ'
        }
        
        for agency, name in agencies.items():
            if agency in text:
                return name
        
        return "AMED"  # Default
    
    def _extract_contact_department(self, text):
        """Extract contact department."""
        patterns = [
            r'(å›½éš›[^ã€‚]*éƒ¨[^ã€‚]*èª²)',
            r'(å›½éš›[^ã€‚]*èª²)',
            r'([^ã€‚]*å›½éš›[^ã€‚]*éƒ¨)',
            r'é€£çµ¡å…ˆ[:ï¼š]\s*([^ã€‚\n]*éƒ¨[^ã€‚\n]*)',
            r'å•ã„?åˆã‚ã›[:ï¼š]\s*([^ã€‚\n]*éƒ¨[^ã€‚\n]*)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1).strip()
        
        return "å›½éš›æˆ¦ç•¥æ¨é€²éƒ¨"  # Default for AMED
    
    def _extract_contact_person(self, text):
        """Extract contact person if available."""
        patterns = [
            r'æ‹…å½“[:ï¼š]\s*([^\s\n]{2,6}[^\s\n]*)',
            r'é€£çµ¡å…ˆ[:ï¼š][^ã€‚]*?([^\s]{2,6}[^\s]*æ§˜?)',
            r'å•ã„?åˆã‚ã›å…ˆ[:ï¼š][^ã€‚]*?([^\s]{2,6}[^\s]*æ‹…å½“)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                name = match.group(1).strip()
                if len(name) < 10 and not any(char in name for char in ['é›»è©±', 'ãƒ¡ãƒ¼ãƒ«', 'TEL', '@']):
                    return name
        
        return "ä¸æ˜"
    
    def _extract_email(self, text):
        """Extract email address."""
        patterns = [
            r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
            r'E-?mail[:ï¼š]\s*([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
            r'ãƒ¡ãƒ¼ãƒ«[:ï¼š]\s*([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                email = match.group(1)
                # Replace common obfuscations
                email = email.replace('"AT"', '@').replace('(at)', '@').replace('[at]', '@')
                return email
        
        return "ä¸æ˜"
    
    def _extract_phone(self, text):
        """Extract phone number."""
        patterns = [
            r'TEL[:ï¼š]\s*([\d\-\(\)]+)',
            r'é›»è©±[:ï¼š]\s*([\d\-\(\)]+)',
            r'(\d{2,4}-\d{2,4}-\d{4})',
            r'(\(\d{2,4}\)\s*\d{2,4}-\d{4})'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1)
        
        return "ä¸æ˜"
    
    def _extract_inquiry_contact(self, text):
        """Extract general inquiry information."""
        patterns = [
            r'å•ã„?åˆã‚ã›å…ˆ[:ï¼š]\s*([^ã€‚\n]{10,100})',
            r'é€£çµ¡å…ˆ[:ï¼š]\s*([^ã€‚\n]{10,100})',
            r'ãŠå•ã„?åˆã‚ã›[:ï¼š]\s*([^ã€‚\n]{10,100})'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                contact = match.group(1).strip()
                if len(contact) > 10:
                    return contact
        
        return "AMEDå…¬å¼ã‚µã‚¤ãƒˆã‚’å‚ç…§"
    
    def _extract_eligibility(self, text):
        """Extract eligibility criteria."""
        patterns = [
            r'å¿œå‹Ÿè³‡æ ¼[:ï¼š]\s*([^ã€‚\n]{10,200})',
            r'è³‡æ ¼[:ï¼š]\s*([^ã€‚\n]{10,200})',
            r'ç”³è«‹è€….*?è¦ä»¶[:ï¼š]\s*([^ã€‚\n]{10,200})'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1).strip()
        
        return "è©³ç´°ã¯å…¬å‹Ÿè¦é …ã‚’å‚ç…§"
    
    def _extract_selection_method(self, text):
        """Extract selection method."""
        if 'æ›¸é¢å¯©æŸ»' in text and 'é¢æ¥' in text:
            return "æ›¸é¢å¯©æŸ» + é¢æ¥å¯©æŸ»"
        elif 'æ›¸é¢å¯©æŸ»' in text:
            return "æ›¸é¢å¯©æŸ»"
        elif 'é¢æ¥' in text:
            return "é¢æ¥å¯©æŸ»"
        else:
            return "å¯©æŸ»æ–¹æ³•ã¯å…¬å‹Ÿè¦é …ã‚’å‚ç…§"
    
    def _extract_required_documents(self, text):
        """Extract required documents."""
        if 'ç”³è«‹æ›¸' in text:
            docs = ["ç”³è«‹æ›¸"]
            if 'CV' in text or 'å±¥æ­´æ›¸' in text:
                docs.append("ç ”ç©¶è€…å±¥æ­´æ›¸")
            if 'è¨ˆç”»æ›¸' in text:
                docs.append("ç ”ç©¶è¨ˆç”»æ›¸")
            if 'äºˆç®—' in text:
                docs.append("äºˆç®—æ›¸")
            return ", ".join(docs)
        
        return "è©³ç´°ã¯å…¬å‹Ÿè¦é …ã‚’å‚ç…§"

def create_comprehensive_excel(studies, filename):
    """Create comprehensive Excel file with all data."""
    
    # Create DataFrame
    df = pd.DataFrame(studies)
    
    # Create workbook
    wb = Workbook()
    ws = wb.active
    ws.title = "å›½éš›ç ”ç©¶ä¸€è¦§"
    
    # Title
    ws.merge_cells('A1:P1')
    ws['A1'] = f"AMED å›½éš›ç ”ç©¶å…¬å‹Ÿæƒ…å ± å®Œå…¨ç‰ˆï¼ˆ{len(studies)}ä»¶ï¼‰"
    ws['A1'].font = Font(size=16, bold=True)
    ws['A1'].alignment = Alignment(horizontal='center')
    
    # Summary
    ws['A3'] = f"ç·ä»¶æ•°: {len(studies)}ä»¶"
    ws['A3'].font = Font(bold=True)
    ws['A4'] = f"ãƒ‡ãƒ¼ã‚¿å–å¾—æ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}"
    
    # Headers
    if not df.empty:
        headers = list(df.columns)
        for col, header in enumerate(headers, 1):
            cell = ws.cell(row=6, column=col, value=header)
            cell.font = Font(bold=True, color="FFFFFF")
            cell.fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
            cell.alignment = Alignment(horizontal='center')
        
        # Data
        for row_idx, row in enumerate(dataframe_to_rows(df, index=False, header=False), 7):
            for col_idx, value in enumerate(row, 1):
                cell = ws.cell(row=row_idx, column=col_idx, value=value)
                
                # Status-based coloring
                if col_idx < len(headers) and 'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹' in headers:
                    status_col = headers.index('ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹') + 1
                    if col_idx == status_col and value == 'ç¾åœ¨å…¬å‹Ÿä¸­':
                        cell.fill = PatternFill(start_color="E8F5E8", end_color="E8F5E8", fill_type="solid")
                
                if row_idx % 2 == 0 and cell.fill.start_color.rgb != "FFE8F5E8":
                    cell.fill = PatternFill(start_color="F8F8F8", end_color="F8F8F8", fill_type="solid")
        
        # Adjust column widths
        for col in range(1, len(headers) + 1):
            ws.column_dimensions[chr(64 + col)].width = 20
    
    # Save
    wb.save(filename)
    print(f"âœ… Excelä¿å­˜å®Œäº†: {filename}")

def main():
    """Main execution function."""
    print("ğŸš€ AMEDå›½éš›ç ”ç©¶ 258ä»¶å®Œå…¨åé›†é–‹å§‹")
    print("=" * 60)
    
    collector = AMEDDataCollector()
    
    # Step 1: Search all international studies
    print("\nğŸ“‹ ã‚¹ãƒ†ãƒƒãƒ—1: å›½éš›ç ”ç©¶ã®æ¤œç´¢")
    studies = collector.search_all_international()
    
    if not studies:
        print("âŒ ç ”ç©¶ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    # Step 2: Enrich with details and contact info
    print(f"\nğŸ“‹ ã‚¹ãƒ†ãƒƒãƒ—2: è©³ç´°æƒ…å ±ã¨ã‚³ãƒ³ã‚¿ã‚¯ãƒˆæƒ…å ±ã®å–å¾—")
    enriched_studies = collector.enrich_with_details(studies)
    
    # Step 3: Create comprehensive Excel
    print(f"\nğŸ“‹ ã‚¹ãƒ†ãƒƒãƒ—3: å®Œå…¨ç‰ˆExcelãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ")
    timestamp = datetime.now().strftime('%Y%m%d_%H%M')
    filename = f"data/AMED_å›½éš›ç ”ç©¶_å®Œå…¨ç‰ˆ258ä»¶_{timestamp}.xlsx"
    
    create_comprehensive_excel(enriched_studies, filename)
    
    # Summary
    print(f"\nğŸ“Š åé›†å®Œäº†ã‚µãƒãƒªãƒ¼:")
    print(f"  ç·ä»¶æ•°: {len(enriched_studies)}ä»¶")
    
    # Status breakdown
    if enriched_studies and 'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹' in enriched_studies[0]:
        status_counts = {}
        for study in enriched_studies:
            status = study.get('ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹', 'ä¸æ˜')
            status_counts[status] = status_counts.get(status, 0) + 1
        
        for status, count in status_counts.items():
            print(f"  {status}: {count}ä»¶")
    
    # Program breakdown
    if enriched_studies and 'ãƒ—ãƒ­ã‚°ãƒ©ãƒ ç¨®åˆ¥' in enriched_studies[0]:
        program_counts = {}
        for study in enriched_studies:
            program = study.get('ãƒ—ãƒ­ã‚°ãƒ©ãƒ ç¨®åˆ¥', 'ä¸æ˜')
            program_counts[program] = program_counts.get(program, 0) + 1
        
        print(f"\nğŸ“ˆ ãƒ—ãƒ­ã‚°ãƒ©ãƒ åˆ¥:")
        for program, count in sorted(program_counts.items(), key=lambda x: x[1], reverse=True):
            print(f"  {program}: {count}ä»¶")
    
    print(f"\nğŸ“‚ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {filename}")
    print("\nâœ… å®Œå…¨åé›†å®Œäº†ï¼")

if __name__ == "__main__":
    main()