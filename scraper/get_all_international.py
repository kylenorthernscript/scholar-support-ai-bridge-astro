#!/usr/bin/env python3
"""Get all 258 international studies from AMED."""

import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import re
import time
from urllib.parse import urlencode
from openpyxl import Workbook
from openpyxl.styles import Font, PatternFill, Alignment
from openpyxl.utils.dataframe import dataframe_to_rows

def get_all_international_studies():
    """Get all international studies by scraping search results."""
    base_url = "https://www.amed.go.jp/search.php"
    
    # Search for all international studies (past + current)
    params = {
        'keyword': 'å›½éš›',
        'search': 'search',
        'order_by': '',
        'stage[]': ['ç¾åœ¨å…¬å‹Ÿä¸­', 'å…¬å‹Ÿæƒ…å ±ï¼ˆéå»ã®å…¬å‹Ÿæƒ…å ±ã‚‚å«ã‚€ï¼‰']
    }
    
    headers = {
        "User-Agent": "Mozilla/5.0 (compatible; ResearchBot/1.0)",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
    }
    
    all_studies = []
    page = 1
    max_pages = 30  # Should be enough for 258 results (10 per page = 26 pages)
    
    print("ğŸ” AMEDå›½éš›ç ”ç©¶ã®å…¨ä»¶å–å¾—ã‚’é–‹å§‹...")
    
    while page <= max_pages:
        print(f"ğŸ“„ ãƒšãƒ¼ã‚¸ {page} ã‚’å–å¾—ä¸­...")
        
        try:
            # Add page parameter
            current_params = params.copy()
            if page > 1:
                current_params['page'] = page
            
            # Build URL
            url = f"{base_url}?{urlencode(current_params, doseq=True)}"
            
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'lxml')
            
            # Check total count on first page
            if page == 1:
                count_text = soup.get_text()
                match = re.search(r'æ¤œç´¢çµæœ.*?(\d+)ä»¶ä¸­', count_text)
                if match:
                    total_count = int(match.group(1))
                    print(f"ğŸ“Š ç·ä»¶æ•°: {total_count}ä»¶")
                    max_pages = min(max_pages, (total_count // 10) + 2)
            
            # Find result items - try multiple selectors
            result_items = []
            
            # Try different possible selectors for search results
            selectors = [
                'div.searchResult',
                'div.searchResultItem', 
                'li.searchResult',
                'tr.searchResult',
                'div.result',
                'li.result'
            ]
            
            for selector in selectors:
                items = soup.select(selector)
                if items:
                    result_items = items
                    print(f"   è¦‹ã¤ã‹ã£ãŸã‚»ãƒ¬ã‚¯ã‚¿: {selector} ({len(items)}ä»¶)")
                    break
            
            # If no structured results, look for links
            if not result_items:
                print("   æ§‹é€ åŒ–çµæœãŒè¦‹ã¤ã‹ã‚‰ãªã„ - ãƒªãƒ³ã‚¯ã‚’ç›´æ¥æ¤œç´¢")
                links = soup.find_all('a', href=re.compile(r'/koubo/'))
                
                current_page_studies = []
                for link in links:
                    href = link.get('href', '')
                    title = link.get_text(strip=True)
                    
                    # Filter for valid recruitment links
                    if (title and len(title) > 10 and 
                        'ä»¤å’Œ' in title and 
                        'å›½éš›' in title and
                        not any(skip in href for skip in ['index', 'search', 'help'])):
                        
                        # Avoid duplicates
                        if not any(study['ã‚¿ã‚¤ãƒˆãƒ«'] == title for study in all_studies):
                            study_data = parse_study_from_link(link, title, href)
                            if study_data:
                                current_page_studies.append(study_data)
                
                if current_page_studies:
                    all_studies.extend(current_page_studies)
                    print(f"   ã“ã®ãƒšãƒ¼ã‚¸ã§{len(current_page_studies)}ä»¶å–å¾—")
                else:
                    print(f"   ã“ã®ãƒšãƒ¼ã‚¸ã§ã¯æ–°ã—ã„çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    break
            
            else:
                # Parse structured result items
                current_page_studies = []
                for item in result_items:
                    study_data = parse_study_from_item(item)
                    if study_data:
                        current_page_studies.append(study_data)
                
                if current_page_studies:
                    all_studies.extend(current_page_studies)
                    print(f"   ã“ã®ãƒšãƒ¼ã‚¸ã§{len(current_page_studies)}ä»¶å–å¾—")
                else:
                    break
            
            # Rate limiting
            time.sleep(1)
            page += 1
            
        except Exception as e:
            print(f"âŒ ãƒšãƒ¼ã‚¸ {page} ã§ã‚¨ãƒ©ãƒ¼: {e}")
            break
    
    print(f"\nâœ… å–å¾—å®Œäº†: {len(all_studies)}ä»¶")
    return all_studies

def parse_study_from_link(link, title, href):
    """Parse study data from a link element."""
    try:
        # Get surrounding context
        parent = link.parent
        if parent:
            context = parent.get_text()
        else:
            context = title
        
        # Extract year from title
        year_match = re.search(r'ä»¤å’Œ(\d+)å¹´', title)
        year = None
        if year_match:
            year = 2018 + int(year_match.group(1))
        
        # Determine program type
        program_type = "ä¸æ˜"
        if "ASPIRE" in title:
            program_type = "ASPIRE"
        elif "SICORP" in title:
            program_type = "SICORP"  
        elif "e-ASIA" in title:
            program_type = "e-ASIA"
        elif "Interstellar" in title:
            program_type = "Interstellar Initiative"
        elif "SATREPS" in title:
            program_type = "SATREPS"
        elif "æ—¥ç±³" in title:
            program_type = "æ—¥ç±³åŒ»å­¦å”åŠ›"
        elif "æµ·å¤–" in title:
            program_type = "æµ·å¤–æ‹ ç‚¹æ´»ç”¨"
        elif "ã‚°ãƒ­ãƒ¼ãƒãƒ«" in title:
            program_type = "ã‚°ãƒ­ãƒ¼ãƒãƒ«å±•é–‹"
        
        # Extract countries
        countries = extract_countries_from_title(title)
        
        # Determine status
        status = "å‹Ÿé›†çµ‚äº†" if year and year < 2025 else "ç¢ºèªè¦"
        
        return {
            'ã‚¿ã‚¤ãƒˆãƒ«': title,
            'URL': f"https://www.amed.go.jp{href}" if href.startswith('/') else href,
            'å¹´åº¦': f"ä»¤å’Œ{year-2018}å¹´" if year else "ä¸æ˜",
            'ãƒ—ãƒ­ã‚°ãƒ©ãƒ ç¨®åˆ¥': program_type,
            'å¯¾è±¡å›½': countries,
            'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹': status,
            'å–å¾—æ—¥æ™‚': datetime.now().strftime('%Y/%m/%d %H:%M')
        }
        
    except Exception as e:
        print(f"Warning: ãƒ‘ãƒ¼ã‚¹ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def parse_study_from_item(item):
    """Parse study data from a structured result item."""
    try:
        # Find link within item
        link = item.find('a', href=re.compile(r'/koubo/'))
        if not link:
            return None
        
        title = link.get_text(strip=True)
        href = link.get('href', '')
        
        return parse_study_from_link(link, title, href)
        
    except Exception as e:
        print(f"Warning: æ§‹é€ åŒ–ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒ‘ãƒ¼ã‚¹ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def extract_countries_from_title(title):
    """Extract countries from title."""
    country_patterns = {
        "æ—¥ãƒ»ã‚«ãƒŠãƒ€": "æ—¥æœ¬, ã‚«ãƒŠãƒ€",
        "æ—¥ãƒ»ã‚¹ã‚¤ã‚¹": "æ—¥æœ¬, ã‚¹ã‚¤ã‚¹", 
        "æ—¥ãƒ»è‹±å›½": "æ—¥æœ¬, è‹±å›½",
        "æ—¥ãƒ»ãƒ•ãƒ©ãƒ³ã‚¹": "æ—¥æœ¬, ãƒ•ãƒ©ãƒ³ã‚¹",
        "æ—¥ãƒ»ãƒ‰ã‚¤ãƒ„": "æ—¥æœ¬, ãƒ‰ã‚¤ãƒ„",
        "æ—¥ãƒ»ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢": "æ—¥æœ¬, ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢",
        "æ—¥ãƒ»ã‚·ãƒ³ã‚¬ãƒãƒ¼ãƒ«": "æ—¥æœ¬, ã‚·ãƒ³ã‚¬ãƒãƒ¼ãƒ«",
        "æ—¥ãƒ»å—ã‚¢ãƒ•ãƒªã‚«": "æ—¥æœ¬, å—ã‚¢ãƒ•ãƒªã‚«",
        "æ—¥ãƒ»ãƒªãƒˆã‚¢ãƒ‹ã‚¢": "æ—¥æœ¬, ãƒªãƒˆã‚¢ãƒ‹ã‚¢",
        "æ—¥ãƒ»åŒ—æ¬§": "æ—¥æœ¬, åŒ—æ¬§è«¸å›½",
        "æ—¥ç±³": "æ—¥æœ¬, ã‚¢ãƒ¡ãƒªã‚«",
        "æ—¥æ¬§": "æ—¥æœ¬, æ¬§å·",
        "æ—¥ä¸­": "æ—¥æœ¬, ä¸­å›½",
        "æ—¥éŸ“": "æ—¥æœ¬, éŸ“å›½",
        "e-ASIA": "æ—¥æœ¬, ã‚¢ã‚¸ã‚¢è«¸å›½",
        "æµ·å¤–": "æµ·å¤–",
        "ã‚°ãƒ­ãƒ¼ãƒãƒ«": "å›½éš›",
        "ã‚¢ãƒ•ãƒªã‚«": "ã‚¢ãƒ•ãƒªã‚«"
    }
    
    for pattern, countries in country_patterns.items():
        if pattern in title:
            return countries
    
    return "è¤‡æ•°å›½/ä¸æ˜"

def create_excel_with_formatting(df, filename, title):
    """Create Excel file with formatting."""
    # Create workbook and worksheet
    wb = Workbook()
    ws = wb.active
    ws.title = "å›½éš›ç ”ç©¶ä¸€è¦§"
    
    # Add title
    ws.merge_cells('A1:G1')
    ws['A1'] = title
    ws['A1'].font = Font(size=16, bold=True)
    ws['A1'].alignment = Alignment(horizontal='center')
    
    # Add summary
    ws['A3'] = f"ç·ä»¶æ•°: {len(df)}ä»¶"
    ws['A3'].font = Font(bold=True)
    ws['A4'] = f"ãƒ‡ãƒ¼ã‚¿å–å¾—æ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}"
    
    # Add headers
    headers = list(df.columns)
    for col, header in enumerate(headers, 1):
        cell = ws.cell(row=6, column=col, value=header)
        cell.font = Font(bold=True, color="FFFFFF")
        cell.fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
        cell.alignment = Alignment(horizontal='center')
    
    # Add data
    for row_idx, row in enumerate(dataframe_to_rows(df, index=False, header=False), 7):
        for col_idx, value in enumerate(row, 1):
            cell = ws.cell(row=row_idx, column=col_idx, value=value)
            
            # Alternate row colors
            if row_idx % 2 == 0:
                cell.fill = PatternFill(start_color="F2F2F2", end_color="F2F2F2", fill_type="solid")
    
    # Adjust column widths
    column_widths = [80, 50, 12, 25, 25, 15, 20]
    for i, width in enumerate(column_widths, 1):
        ws.column_dimensions[chr(64 + i)].width = width
    
    # Add summary sheet
    ws2 = wb.create_sheet("ãƒ—ãƒ­ã‚°ãƒ©ãƒ åˆ¥é›†è¨ˆ")
    
    # Program type summary
    program_summary = df['ãƒ—ãƒ­ã‚°ãƒ©ãƒ ç¨®åˆ¥'].value_counts()
    ws2['A1'] = "ãƒ—ãƒ­ã‚°ãƒ©ãƒ ç¨®åˆ¥åˆ¥ä»¶æ•°"
    ws2['A1'].font = Font(bold=True, size=14)
    
    row = 3
    for program, count in program_summary.items():
        ws2[f'A{row}'] = program
        ws2[f'B{row}'] = count
        row += 1
    
    # Save
    wb.save(filename)
    print(f"âœ… Excelä¿å­˜å®Œäº†: {filename}")

def main():
    """Main execution function."""
    # Get all international studies
    all_studies = get_all_international_studies()
    
    if not all_studies:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    # Create DataFrame
    df = pd.DataFrame(all_studies)
    
    # Remove duplicates based on title
    df = df.drop_duplicates(subset=['ã‚¿ã‚¤ãƒˆãƒ«'])
    
    print(f"ğŸ“Š é‡è¤‡é™¤å»å¾Œ: {len(df)}ä»¶")
    
    # Create Excel file
    timestamp = datetime.now().strftime('%Y%m%d_%H%M')
    filename = f"data/AMED_å›½éš›ç ”ç©¶_å…¨ä»¶_{timestamp}.xlsx"
    
    create_excel_with_formatting(
        df,
        filename, 
        f"AMED å›½éš›ç ”ç©¶å…¬å‹Ÿæƒ…å ± å…¨{len(df)}ä»¶"
    )
    
    # Show summary
    print(f"\nğŸ“ˆ ãƒ—ãƒ­ã‚°ãƒ©ãƒ åˆ¥é›†è¨ˆ:")
    program_counts = df['ãƒ—ãƒ­ã‚°ãƒ©ãƒ ç¨®åˆ¥'].value_counts()
    for program, count in program_counts.items():
        print(f"  {program}: {count}ä»¶")
    
    print(f"\nğŸ“‚ ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«: {filename}")

if __name__ == "__main__":
    main()